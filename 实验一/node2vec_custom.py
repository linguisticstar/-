# node2vec_custom.py
import pandas as pd
import networkx as nx
import numpy as np
import random
from gensim.models import Word2Vec
from collections import defaultdict
import re
import os
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
from tqdm import tqdm
import multiprocessing
import time

# ==============================
# é…ç½®å‚æ•°
# ==============================
DATA_FILE = "postings.csv"
SKILLS = [
    "python", "java", "javascript", "sql", "machine learning", "deep learning",
    "data analysis", "cloud computing", "aws", "azure", "docker", "kubernetes",
    "react", "nodejs", "html", "css", "git", "agile", "scrum", "project management",
    "communication", "leadership", "problem solving", "teamwork", "analytical skills"
]
SKILLS_LOWER = {s.lower(): s for s in SKILLS}


# ==============================
# è‡ªå®šä¹‰ Node2Vec å®ç°
# ==============================
class CustomNode2Vec:
    def __init__(self, graph, walk_length=30, num_walks=100, p=1.0, q=1.0, workers=1):
        self.graph = graph
        self.walk_length = walk_length
        self.num_walks = num_walks
        self.p = p
        self.q = q
        self.workers = workers

    def random_walk(self, start_node):
        """æ‰§è¡Œéšæœºæ¸¸èµ°"""
        walk = [start_node]
        current_node = start_node

        for _ in range(self.walk_length - 1):
            neighbors = list(self.graph.neighbors(current_node))
            if not neighbors:
                break

            # ç®€å•çš„éšæœºæ¸¸èµ°ï¼ˆå¯ä»¥æ‰©å±•ä¸ºnode2vecçš„biased walkï¼‰
            next_node = random.choice(neighbors)
            walk.append(next_node)
            current_node = next_node

        return walk

    def generate_walks_parallel(self):
        """å¹¶è¡Œç”Ÿæˆéšæœºæ¸¸èµ°åºåˆ—"""
        nodes = list(self.graph.nodes())
        all_walks = []

        # ä½¿ç”¨è¿›ç¨‹æ± å¹¶è¡Œå¤„ç†
        with multiprocessing.Pool(processes=self.workers) as pool:
            # ä¸ºæ¯ä¸ªè¿›ç¨‹åˆ†é…ä»»åŠ¡
            for walk_num in tqdm(range(self.num_walks), desc="éšæœºæ¸¸èµ°è¿›åº¦"):
                random.shuffle(nodes)

                # å¹¶è¡Œç”Ÿæˆæ¸¸èµ°åºåˆ—
                walks = pool.map(self.random_walk, nodes)
                all_walks.extend(walks)

        return all_walks

    def generate_walks(self):
        """ä¸ºæ‰€æœ‰èŠ‚ç‚¹ç”Ÿæˆéšæœºæ¸¸èµ°åºåˆ—ï¼ˆå•çº¿ç¨‹ç‰ˆæœ¬ï¼‰"""
        all_walks = []
        nodes = list(self.graph.nodes())

        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
        for walk_num in tqdm(range(self.num_walks), desc="éšæœºæ¸¸èµ°è¿›åº¦"):
            random.shuffle(nodes)

            # å†…éƒ¨è¿›åº¦æ¡
            for node in tqdm(nodes, desc=f"ç¬¬{walk_num + 1}/{self.num_walks}è½®æ¸¸èµ°", leave=False):
                walk = self.random_walk(node)
                all_walks.append(walk)

        return all_walks


# ==============================
# æ•°æ®é¢„å¤„ç†å‡½æ•°
# ==============================
def extract_company(text):
    """æå–å…¬å¸åç§°"""
    if pd.isna(text) or str(text).strip().lower() in ['nan', 'none', '']:
        return None
    text = str(text).strip()

    # å¤„ç†é‚®ç®±æ ¼å¼
    if '@' in text:
        try:
            domain = text.split('@')[1].split('.')[0]
            return domain.title().replace(' ', '')
        except:
            pass

    return text[:50].split('\n')[0].strip()


def extract_skills(text):
    """ä»æ–‡æœ¬ä¸­æå–æŠ€èƒ½"""
    if pd.isna(text):
        return set()

    text_lower = str(text).lower()
    matched_skills = set()

    # åŒ¹é…é¢„å®šä¹‰æŠ€èƒ½
    for skill_lower, skill_orig in SKILLS_LOWER.items():
        if skill_lower in text_lower:
            matched_skills.add(skill_orig)

    return matched_skills


# ==============================
# å¯è§†åŒ–å‡½æ•°
# ==============================
def visualize_embeddings(model, nodes, max_nodes=200):
    """ä½¿ç”¨T-SNEè¿›è¡Œé™ç»´å¯è§†åŒ–"""
    if len(nodes) > max_nodes:
        print(f"èŠ‚ç‚¹è¿‡å¤š({len(nodes)})ï¼Œéšæœºé€‰æ‹©{max_nodes}ä¸ªè¿›è¡Œå¯è§†åŒ–")
        nodes = np.random.choice(nodes, max_nodes, replace=False)

    # è·å–åµŒå…¥å‘é‡
    embeddings = [model.wv[node] for node in nodes]
    embeddings_array = np.array(embeddings)

    # T-SNEé™ç»´
    print("æ­£åœ¨è¿›è¡ŒT-SNEé™ç»´...")
    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(nodes) - 1))
    embeddings_2d = tsne.fit_transform(embeddings_array)

    # åˆ›å»ºå¯è§†åŒ–
    plt.figure(figsize=(15, 10))

    # åŒºåˆ†å…¬å¸å’ŒæŠ€èƒ½èŠ‚ç‚¹
    companies = [node for node in nodes if node not in SKILLS]
    skills = [node for node in nodes if node in SKILLS]

    # ç»˜åˆ¶å…¬å¸èŠ‚ç‚¹
    company_indices = [i for i, node in enumerate(nodes) if node in companies]
    if company_indices:
        plt.scatter(embeddings_2d[company_indices, 0],
                    embeddings_2d[company_indices, 1],
                    c='blue', alpha=0.6, label='Companies', s=50)

        # æ ‡æ³¨ä¸€äº›å…¬å¸
        for i in company_indices[:10]:  # åªæ ‡æ³¨å‰10ä¸ªå…¬å¸
            plt.annotate(nodes[i],
                         (embeddings_2d[i, 0], embeddings_2d[i, 1]),
                         xytext=(5, 5), textcoords='offset points',
                         fontsize=8, alpha=0.8)

    # ç»˜åˆ¶æŠ€èƒ½èŠ‚ç‚¹
    skill_indices = [i for i, node in enumerate(nodes) if node in skills]
    if skill_indices:
        plt.scatter(embeddings_2d[skill_indices, 0],
                    embeddings_2d[skill_indices, 1],
                    c='red', alpha=0.6, label='Skills', s=50)

        # æ ‡æ³¨æŠ€èƒ½
        for i in skill_indices:
            plt.annotate(nodes[i],
                         (embeddings_2d[i, 0], embeddings_2d[i, 1]),
                         xytext=(5, 5), textcoords='offset points',
                         fontsize=8, alpha=0.8)

    plt.title('Custom Node2Vec Embeddings Visualization (T-SNE)')
    plt.xlabel('TSNE Component 1')
    plt.ylabel('TSNE Component 2')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # ä¿å­˜å›¾åƒ
    plt.savefig('custom_node2vec_tsne.png', dpi=300, bbox_inches='tight')
    plt.show()

    print("âœ… å¯è§†åŒ–å·²ä¿å­˜: custom_node2vec_tsne.png")


# ==============================
# ä¸»ç¨‹åº
# ==============================
def main():
    print("ğŸ” æ­£åœ¨åŠ è½½ LinkedIn èŒä½æ•°æ®...")

    try:
        # å°è¯•ä¸åŒçš„ç¼–ç æ–¹å¼
        try:
            df = pd.read_csv(DATA_FILE, on_bad_lines='skip', low_memory=False)
        except:
            try:
                df = pd.read_csv(DATA_FILE, encoding='latin-1', on_bad_lines='skip', low_memory=False)
            except:
                # å¦‚æœå‰ä¸¤åˆ—æ˜¯å…¬å¸ä¿¡æ¯ï¼Œç›´æ¥è¯»å–å‰å‡ åˆ—
                df = pd.read_csv(DATA_FILE, usecols=[0, 1, 2], on_bad_lines='skip', low_memory=False)

        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {df.shape}")
        if len(df.columns) < 3:
            print(f"æ•°æ®åˆ—: {df.columns.tolist()}")
        else:
            print("ä½¿ç”¨å‰ä¸‰åˆ—æ•°æ®è¿›è¡Œå¤„ç†")

    except FileNotFoundError:
        print(f"âŒ æ–‡ä»¶ '{DATA_FILE}' æœªæ‰¾åˆ°!")
        print("è¯·ç¡®ä¿æ–‡ä»¶åœ¨å½“å‰ç›®å½•ï¼Œæˆ–ä»ä»¥ä¸‹é“¾æ¥ä¸‹è½½:")
        print("https://www.kaggle.com/datasets/arshkon/linkedin-job-postings")

        # åˆ›å»ºç¤ºä¾‹æ•°æ®ç”¨äºæ¼”ç¤º
        print("\nâš ï¸ ä½¿ç”¨ç¤ºä¾‹æ•°æ®è¿›è¡Œæ¼”ç¤º...")
        sample_data = {
            'company': ['Google', 'Microsoft', 'Amazon', 'Apple', 'Meta'],
            'description': [
                'Looking for python java machine learning experts',
                'Need java sql cloud computing professionals',
                'Hiring for aws docker kubernetes engineers',
                'Seeking react javascript html css developers',
                'Want python data analysis machine learning talent'
            ]
        }
        df = pd.DataFrame(sample_data)
        print("ç¤ºä¾‹æ•°æ®å·²åˆ›å»º")

    # æ„å»ºå…¬å¸-æŠ€èƒ½å…³ç³»å›¾
    print("\nğŸ“Š æ­£åœ¨æ„å»ºå…¬å¸-æŠ€èƒ½å…³ç³»å›¾...")
    edges = []

    for idx, row in tqdm(df.iterrows(), total=len(df), desc="å¤„ç†æ•°æ®è¡Œ"):
        # æå–å…¬å¸åç§°
        company = None
        for col_idx in range(min(3, len(row))):
            company = extract_company(row.iloc[col_idx])  # ä¿®å¤è­¦å‘Š
            if company:
                break

        if not company:
            continue

        # åˆå¹¶æ‰€æœ‰æ–‡æœ¬å†…å®¹è¿›è¡ŒæŠ€èƒ½æå–
        full_text = " ".join([str(x) for x in row if pd.notna(x)])
        skills = extract_skills(full_text)

        # æ·»åŠ è¾¹
        for skill in skills:
            edges.append((company, skill))

    print(f"âœ… å›¾æ„å»ºå®Œæˆ: {len(edges)} æ¡è¾¹")

    if len(edges) == 0:
        print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•å…³ç³»è¾¹ï¼Œä½¿ç”¨ç¤ºä¾‹æ•°æ®")
        # æ·»åŠ ä¸€äº›ç¤ºä¾‹è¾¹
        edges = [
            ('Google', 'python'), ('Google', 'java'), ('Google', 'machine learning'),
            ('Microsoft', 'java'), ('Microsoft', 'sql'), ('Microsoft', 'cloud computing'),
            ('Amazon', 'aws'), ('Amazon', 'docker'), ('Amazon', 'kubernetes'),
            ('Apple', 'react'), ('Apple', 'javascript'), ('Apple', 'html'),
            ('Meta', 'python'), ('Meta', 'data analysis'), ('Meta', 'machine learning')
        ]

    # åˆ›å»ºå›¾
    edge_df = pd.DataFrame(edges, columns=["source", "target"])
    G = nx.from_pandas_edgelist(edge_df, "source", "target", create_using=nx.Graph())

    print(f"ğŸ“ˆ å›¾ç»Ÿè®¡: {G.number_of_nodes()} ä¸ªèŠ‚ç‚¹, {G.number_of_edges()} æ¡è¾¹")

    # ä½¿ç”¨è‡ªå®šä¹‰Node2Vecè®­ç»ƒæ¨¡å‹
    print("\nğŸ§  æ­£åœ¨è®­ç»ƒè‡ªå®šä¹‰ Node2Vec æ¨¡å‹...")

    # æ ¹æ®æ•°æ®å¤§å°è°ƒæ•´å‚æ•°
    if G.number_of_nodes() > 50000:
        walk_length = 30
        num_walks = 100
        workers = 20  # ä½¿ç”¨20ä¸ªworker

    print(f"å‚æ•°è®¾ç½®: walk_length={walk_length}, num_walks={num_walks}, workers={workers}")

    custom_node2vec = CustomNode2Vec(
        graph=G,
        walk_length=walk_length,
        num_walks=num_walks,
        p=1.0,
        q=1.0,
        workers=workers
    )

    # ç”Ÿæˆéšæœºæ¸¸èµ°åºåˆ—
    print("ç”Ÿæˆéšæœºæ¸¸èµ°åºåˆ—...")

    # æ ¹æ®ç³»ç»Ÿèµ„æºé€‰æ‹©å¹¶è¡Œæˆ–ä¸²è¡Œç‰ˆæœ¬
    if workers > 1:
        print("ä½¿ç”¨å¹¶è¡Œç‰ˆæœ¬ç”Ÿæˆéšæœºæ¸¸èµ°...")
        walks = custom_node2vec.generate_walks_parallel()
    else:
        print("ä½¿ç”¨ä¸²è¡Œç‰ˆæœ¬ç”Ÿæˆéšæœºæ¸¸èµ°...")
        walks = custom_node2vec.generate_walks()

    print(f"ç”Ÿæˆäº† {len(walks)} æ¡éšæœºæ¸¸èµ°åºåˆ—")

    # ä½¿ç”¨Word2Vecè®­ç»ƒèŠ‚ç‚¹åµŒå…¥
    print("è®­ç»ƒWord2Vecæ¨¡å‹...")
    model = Word2Vec(
        sentences=walks,
        vector_size=64,  # åµŒå…¥ç»´åº¦
        window=5,  # ä¸Šä¸‹æ–‡çª—å£
        min_count=1,  # æœ€å°å‡ºç°æ¬¡æ•°
        workers=workers,  # ä½¿ç”¨20ä¸ªworker
        epochs=10
    )

    # ä¿å­˜æ¨¡å‹
    model.save("custom_node2vec.model")
    print("âœ… æ¨¡å‹å·²ä¿å­˜: custom_node2vec.model")

    # ç›¸ä¼¼åº¦è®¡ç®—ç¤ºä¾‹
    print("\nğŸ” ç›¸ä¼¼åº¦è®¡ç®—ç¤ºä¾‹:")

    # æŸ¥æ‰¾æœ‰åµŒå…¥çš„èŠ‚ç‚¹è¿›è¡Œæµ‹è¯•
    nodes_with_embeddings = list(model.wv.key_to_index.keys())
    print(f"æœ‰åµŒå…¥å‘é‡çš„èŠ‚ç‚¹æ•°é‡: {len(nodes_with_embeddings)}")

    if nodes_with_embeddings:
        # æµ‹è¯•å…¬å¸èŠ‚ç‚¹
        company_nodes = [n for n in nodes_with_embeddings if n not in SKILLS]
        if company_nodes:
            test_node = company_nodes[0]
            print(f"\nä¸ '{test_node}' æœ€ç›¸ä¼¼çš„èŠ‚ç‚¹:")
            try:
                similar_nodes = model.wv.most_similar(test_node, topn=5)
                for node, similarity in similar_nodes:
                    node_type = "æŠ€èƒ½" if node in SKILLS else "å…¬å¸"
                    print(f"  {node}: {similarity:.4f} ({node_type})")
            except Exception as e:
                print(f"ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")

        # æµ‹è¯•æŠ€èƒ½èŠ‚ç‚¹
        skill_nodes = [n for n in nodes_with_embeddings if n in SKILLS]
        if skill_nodes:
            test_skill = skill_nodes[0]
            print(f"\nä¸ '{test_skill}' æœ€ç›¸ä¼¼çš„èŠ‚ç‚¹:")
            try:
                similar_nodes = model.wv.most_similar(test_skill, topn=5)
                for node, similarity in similar_nodes:
                    node_type = "æŠ€èƒ½" if node in SKILLS else "å…¬å¸"
                    print(f"  {node}: {similarity:.4f} ({node_type})")
            except Exception as e:
                print(f"ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")

    # T-SNEå¯è§†åŒ–
    print("\nğŸ¨ æ­£åœ¨è¿›è¡ŒT-SNEå¯è§†åŒ–...")
    visualize_embeddings(model, nodes_with_embeddings)


if __name__ == "__main__":
    main()