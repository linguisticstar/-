# data_manager.py - æ•°æ®ç®¡ç†å·¥å…·
import os
import json
import re
import sys
import shutil
from datetime import datetime
from pathlib import Path
from bs4 import BeautifulSoup


def print_header(title):
    """æ‰“å°æ ‡é¢˜"""
    print("\n" + "=" * 60)
    print(f"ğŸ“Š {title}")
    print("=" * 60)


def check_data_directory():
    """æ£€æŸ¥æ•°æ®ç›®å½•"""
    data_dir = "./data"
    if not os.path.exists(data_dir):
        os.makedirs(data_dir)
        print(f"åˆ›å»ºæ•°æ®ç›®å½•: {data_dir}")

    html_files = [f for f in os.listdir(data_dir) if f.endswith('.html')]
    json_file = os.path.join(data_dir, "processed_data.json")

    return html_files, json_file


def generate_sample_html():
    """ç”Ÿæˆç¤ºä¾‹HTMLæ–‡ä»¶"""
    sample_diseases = [
        {
            "name": "é«˜è¡€å‹",
            "symptoms": "å¤´ç—›ã€å¤´æ™•ã€å¿ƒæ‚¸ã€èƒ¸é—·ã€ç–²åŠ³",
            "causes": "é—ä¼ å› ç´ ã€é«˜ç›é¥®é£Ÿã€è‚¥èƒ–ã€ç¼ºä¹è¿åŠ¨ã€å‹åŠ›è¿‡å¤§",
            "treatment": "è¯ç‰©æ²»ç–—ï¼ˆåˆ©å°¿å‰‚ã€Î²å—ä½“é˜»æ»å‰‚ï¼‰ã€ä½ç›é¥®é£Ÿã€å®šæœŸè¿åŠ¨ã€æ§åˆ¶ä½“é‡"
        },
        {
            "name": "ç³–å°¿ç—…",
            "symptoms": "å¤šé¥®ã€å¤šå°¿ã€å¤šé£Ÿã€ä½“é‡ä¸‹é™ã€è§†åŠ›æ¨¡ç³Š",
            "causes": "èƒ°å²›ç´ åˆ†æ³Œä¸è¶³ã€èƒ°å²›ç´ æŠµæŠ—ã€é—ä¼ å› ç´ ã€è‚¥èƒ–",
            "treatment": "èƒ°å²›ç´ æ³¨å°„ã€å£æœé™ç³–è¯ã€é¥®é£Ÿæ§åˆ¶ã€è¿åŠ¨ç–—æ³•"
        },
        {
            "name": "å† å¿ƒç—…",
            "symptoms": "èƒ¸ç—›ã€èƒ¸é—·ã€å¿ƒæ‚¸ã€æ°”çŸ­ã€ç–²åŠ³",
            "causes": "åŠ¨è„‰ç²¥æ ·ç¡¬åŒ–ã€é«˜è¡€å‹ã€é«˜è¡€è„‚ã€å¸çƒŸã€ç³–å°¿ç—…",
            "treatment": "è¯ç‰©æ²»ç–—ï¼ˆé˜¿å¸åŒ¹æ—ã€ä»–æ±€ç±»ï¼‰ã€å† çŠ¶åŠ¨è„‰ä»‹å…¥æ²»ç–—ã€æ­æ¡¥æ‰‹æœ¯"
        },
        {
            "name": "è‚ºç‚",
            "symptoms": "å‘çƒ­ã€å’³å—½ã€å’³ç—°ã€èƒ¸ç—›ã€å‘¼å¸å›°éš¾",
            "causes": "ç»†èŒæ„ŸæŸ“ã€ç—…æ¯’æ„ŸæŸ“ã€çœŸèŒæ„ŸæŸ“ã€å¸å…¥å¼‚ç‰©",
            "treatment": "æŠ—ç”Ÿç´ æ²»ç–—ã€æŠ—ç—…æ¯’è¯ç‰©ã€æ­¢å’³åŒ–ç—°è¯ã€æ°§ç–—"
        },
        {
            "name": "èƒƒç‚",
            "symptoms": "ä¸Šè…¹ç—›ã€è…¹èƒ€ã€æ¶å¿ƒã€å‘•åã€é£Ÿæ¬²ä¸æŒ¯",
            "causes": "å¹½é—¨èºæ†èŒæ„ŸæŸ“ã€è¯ç‰©åˆºæ¿€ã€é¥®é£Ÿä¸å½“ã€å‹åŠ›è¿‡å¤§",
            "treatment": "æŠ—ç”Ÿç´ æ²»ç–—ã€èƒƒé…¸æŠ‘åˆ¶å‰‚ã€ä¿æŠ¤èƒƒé»è†œè¯ç‰©ã€é¥®é£Ÿè°ƒæ•´"
        }
    ]

    data_dir = "./data"
    created_files = []

    for disease in sample_diseases:
        filename = f"{disease['name']} - åŒ»å­¦ç™¾ç§‘.html"
        filepath = os.path.join(data_dir, filename)

        if not os.path.exists(filepath):
            html_content = f"""
            <!DOCTYPE html>
            <html>
            <head>
                <meta charset="UTF-8">
                <title>{disease['name']} - åŒ»å­¦ç™¾ç§‘</title>
            </head>
            <body>
                <h1>{disease['name']}</h1>

                <div class="content">
                    <h2>æ¦‚è¿°</h2>
                    <p>{disease['name']}æ˜¯ä¸€ç§å¸¸è§çš„ç–¾ç—…ï¼Œéœ€è¦åŠæ—¶è¯Šæ–­å’Œæ²»ç–—ã€‚</p>

                    <h2>ç—‡çŠ¶</h2>
                    <p>{disease['symptoms']}</p>

                    <h2>ç—…å› </h2>
                    <p>{disease['causes']}</p>

                    <h2>æ²»ç–—æ–¹æ³•</h2>
                    <p>{disease['treatment']}</p>

                    <h2>é¢„é˜²æªæ–½</h2>
                    <p>1. å¥åº·é¥®é£Ÿ<br>2. è§„å¾‹è¿åŠ¨<br>3. å®šæœŸä½“æ£€<br>4. é¿å…å±é™©å› ç´ </p>

                    <h2>æ³¨æ„äº‹é¡¹</h2>
                    <p>å¦‚å‡ºç°ç›¸å…³ç—‡çŠ¶ï¼Œè¯·åŠæ—¶å°±åŒ»ã€‚æœ¬ä¿¡æ¯ä»…ä¾›å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç–—å»ºè®®ã€‚</p>
                </div>
            </body>
            </html>
            """

            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(html_content)
            created_files.append(filename)
            print(f"âœ… åˆ›å»º: {filename}")

    return created_files


def extract_from_html(html_filepath):
    """ä»HTMLæå–å†…å®¹"""
    try:
        with open(html_filepath, 'r', encoding='utf-8') as f:
            html_content = f.read()

        soup = BeautifulSoup(html_content, 'html.parser')

        # æå–æ ‡é¢˜
        title_tag = soup.find('title')
        title = title_tag.text.strip() if title_tag else Path(html_filepath).stem

        # æå–æ­£æ–‡
        content_tag = soup.find('div', class_='content')
        if not content_tag:
            content_tag = soup.find('body')

        if content_tag:
            text = content_tag.get_text(separator='\n', strip=True)
            # æ¸…ç†æ–‡æœ¬
            text = re.sub(r'\n\s*\n', '\n', text)
            text = re.sub(r'\s+', ' ', text)
            return title, text
        else:
            return title, ""

    except Exception as e:
        print(f"âŒ æå–å¤±è´¥ {html_filepath}: {e}")
        return None, None


def split_text(text, chunk_size=500, chunk_overlap=50):
    """åˆ†å‰²æ–‡æœ¬"""
    if not text or len(text) < 50:
        return []

    chunks = []
    words = text.split()
    current_chunk = []
    current_length = 0

    for word in words:
        word_length = len(word) + 1  # +1 for space

        if current_length + word_length > chunk_size and current_chunk:
            chunks.append(' '.join(current_chunk))

            # ä¿ç•™é‡å éƒ¨åˆ†
            overlap_words = current_chunk[-min(len(current_chunk), chunk_overlap // 5):]
            current_chunk = overlap_words.copy()
            current_length = sum(len(w) + 1 for w in current_chunk)

        current_chunk.append(word)
        current_length += word_length

    # æ·»åŠ æœ€åä¸€ä¸ªå—
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks


def process_html_files():
    """å¤„ç†æ‰€æœ‰HTMLæ–‡ä»¶"""
    data_dir = "./data"
    html_files = [f for f in os.listdir(data_dir) if f.endswith('.html')]

    if not html_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°HTMLæ–‡ä»¶")
        return []

    all_data = []
    total_chunks = 0

    print_header("å¤„ç†HTMLæ–‡ä»¶")
    print(f"æ‰¾åˆ° {len(html_files)} ä¸ªHTMLæ–‡ä»¶")

    for i, filename in enumerate(html_files, 1):
        filepath = os.path.join(data_dir, filename)
        print(f"\n[{i}/{len(html_files)}] å¤„ç†: {filename}")

        title, text = extract_from_html(filepath)

        if text and len(text) > 100:
            chunks = split_text(text)

            for j, chunk in enumerate(chunks):
                data_entry = {
                    "id": f"{filename}_{j}",
                    "title": title or filename,
                    "abstract": chunk,
                    "source_file": filename,
                    "chunk_index": j
                }
                all_data.append(data_entry)

            total_chunks += len(chunks)
            print(f"  â†’ æå–æˆåŠŸ: {len(chunks)} ä¸ªæ–‡æœ¬å—")
        else:
            print(f"  â†’ å†…å®¹è¿‡å°‘æˆ–æ— å†…å®¹ï¼Œè·³è¿‡")

    return all_data, total_chunks


def save_json_data(data, json_filepath):
    """ä¿å­˜JSONæ•°æ®"""
    try:
        with open(json_filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        file_size = os.path.getsize(json_filepath) / 1024  # KB
        print(f"âœ… JSONä¿å­˜æˆåŠŸ: {json_filepath}")
        print(f"   æ•°æ®æ¡æ•°: {len(data)}")
        print(f"   æ–‡ä»¶å¤§å°: {file_size:.1f} KB")

        return True
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
        return False


def backup_chroma_data():
    """å¤‡ä»½ChromaDBæ•°æ®"""
    chroma_dir = "./chroma_data"
    if not os.path.exists(chroma_dir):
        print("â„¹ï¸ ChromaDBæ•°æ®ç›®å½•ä¸å­˜åœ¨ï¼Œæ— éœ€å¤‡ä»½")
        return None

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_dir = f"./backups/chroma_backup_{timestamp}"

    os.makedirs(os.path.dirname(backup_dir), exist_ok=True)

    try:
        shutil.copytree(chroma_dir, backup_dir)
        print(f"âœ… ChromaDBæ•°æ®å¤‡ä»½åˆ°: {backup_dir}")
        return backup_dir
    except Exception as e:
        print(f"âŒ å¤‡ä»½å¤±è´¥: {e}")
        return None


def clear_chroma_data():
    """æ¸…ç©ºChromaDBæ•°æ®"""
    chroma_dir = "./chroma_data"

    if os.path.exists(chroma_dir):
        try:
            shutil.rmtree(chroma_dir)
            os.makedirs(chroma_dir, exist_ok=True)
            print("âœ… ChromaDBæ•°æ®å·²æ¸…ç©º")
            return True
        except Exception as e:
            print(f"âŒ æ¸…ç©ºå¤±è´¥: {e}")
            return False
    else:
        print("â„¹ï¸ ChromaDBæ•°æ®ç›®å½•ä¸å­˜åœ¨ï¼Œæ— éœ€æ¸…ç©º")
        return True


def main_menu():
    """ä¸»èœå•"""
    print_header("åŒ»ç–—RAGç³»ç»Ÿ - æ•°æ®ç®¡ç†å·¥å…·")

    while True:
        print("\nğŸ“‹ ä¸»èœå•:")
        print("  1. ğŸ“ æŸ¥çœ‹æ•°æ®ç»Ÿè®¡")
        print("  2. ğŸ†• ç”Ÿæˆç¤ºä¾‹HTMLæ–‡ä»¶")
        print("  3. ğŸ”„ é‡æ–°å¤„ç†æ‰€æœ‰HTMLæ–‡ä»¶")
        print("  4. ğŸ’¾ å¤‡ä»½ChromaDBæ•°æ®")
        print("  5. ğŸ—‘ï¸  æ¸…ç©ºChromaDBæ•°æ®")
        print("  6. ğŸš€ å¯åŠ¨åº”ç”¨")
        print("  7. ğŸ“¤ å¯¼å‡ºæ•°æ®ç»Ÿè®¡")
        print("  0. ğŸ”š é€€å‡º")

        choice = input("\nè¯·é€‰æ‹©æ“ä½œ (0-7): ").strip()

        if choice == '1':
            view_data_stats()
        elif choice == '2':
            generate_html_menu()
        elif choice == '3':
            reprocess_data_menu()
        elif choice == '4':
            backup_chroma_data()
        elif choice == '5':
            clear_chroma_menu()
        elif choice == '6':
            launch_app()
        elif choice == '7':
            export_stats()
        elif choice == '0':
            print("\nğŸ‘‹ å†è§ï¼")
            break
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")


def view_data_stats():
    """æŸ¥çœ‹æ•°æ®ç»Ÿè®¡"""
    print_header("æ•°æ®ç»Ÿè®¡")

    html_files, json_file = check_data_directory()

    print(f"ğŸ“ HTMLæ–‡ä»¶: {len(html_files)} ä¸ª")
    for i, file in enumerate(sorted(html_files)[:10], 1):
        size = os.path.getsize(f"./data/{file}") / 1024
        print(f"  {i:2d}. {file} ({size:.1f} KB)")

    if len(html_files) > 10:
        print(f"  ... è¿˜æœ‰ {len(html_files) - 10} ä¸ªæ–‡ä»¶")

    if os.path.exists(json_file):
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        print(f"\nğŸ“Š JSONæ•°æ®: {len(data)} æ¡è®°å½•")

        # ç»Ÿè®¡å„æ–‡ä»¶çš„å—æ•°
        from collections import Counter
        sources = Counter([item.get('source_file', 'æœªçŸ¥') for item in data])
        print("\nğŸ“ˆ å„æ–‡ä»¶æ–‡æœ¬å—åˆ†å¸ƒ:")
        for source, count in sources.most_common(10):
            print(f"  {source[:30]:30} : {count:3d} å—")
    else:
        print("\nâŒ JSONæ–‡ä»¶ä¸å­˜åœ¨")

    # æ£€æŸ¥ChromaDB
    if os.path.exists("./chroma_data"):
        print(f"\nğŸ—„ï¸  ChromaDBæ•°æ®ç›®å½•: å­˜åœ¨")
    else:
        print(f"\nğŸ—„ï¸  ChromaDBæ•°æ®ç›®å½•: ä¸å­˜åœ¨")


def generate_html_menu():
    """ç”ŸæˆHTMLæ–‡ä»¶èœå•"""
    print_header("ç”Ÿæˆç¤ºä¾‹HTMLæ–‡ä»¶")

    created = generate_sample_html()
    if created:
        print(f"\nâœ… å·²åˆ›å»º {len(created)} ä¸ªç¤ºä¾‹æ–‡ä»¶")
    else:
        print("â„¹ï¸ æ‰€æœ‰ç¤ºä¾‹æ–‡ä»¶å·²å­˜åœ¨")


def reprocess_data_menu():
    """é‡æ–°å¤„ç†æ•°æ®èœå•"""
    print_header("é‡æ–°å¤„ç†æ•°æ®")

    # å¤‡ä»½ç¡®è®¤
    print("âš ï¸  é‡æ–°å¤„ç†å°†ç”Ÿæˆæ–°çš„JSONæ–‡ä»¶ï¼Œå¯èƒ½ä¼šè¦†ç›–æ—§æ•°æ®")
    confirm = input("æ˜¯å¦ç»§ç»­? (y/N): ").lower()

    if confirm != 'y':
        print("âŒ å·²å–æ¶ˆ")
        return

    # å¤„ç†æ•°æ®
    all_data, total_chunks = process_html_files()

    if not all_data:
        print("âŒ æœªç”Ÿæˆä»»ä½•æ•°æ®")
        return

    # ä¿å­˜JSON
    json_filepath = "./data/processed_data.json"
    if save_json_data(all_data, json_filepath):
        print(f"\nâœ… æ•°æ®å¤„ç†å®Œæˆ!")
        print(f"   æ€»æ–‡æœ¬å—æ•°: {total_chunks}")
        print(f"   æ€»æ•°æ®æ¡ç›®: {len(all_data)}")

        # å»ºè®®æ¸…ç©ºChromaDB
        print("\nğŸ’¡ å»ºè®®: å¤„ç†å®Œæ–°æ•°æ®åï¼Œå»ºè®®æ¸…ç©ºChromaDBæ•°æ®å¹¶é‡å¯åº”ç”¨")
    else:
        print("âŒ æ•°æ®å¤„ç†å¤±è´¥")


def clear_chroma_menu():
    """æ¸…ç©ºChromaDBèœå•"""
    print_header("æ¸…ç©ºChromaDBæ•°æ®")

    print("âš ï¸  æ¸…ç©ºåéœ€è¦é‡æ–°ç´¢å¼•æ‰€æœ‰æ•°æ®")
    confirm = input("ç¡®è®¤æ¸…ç©º? (y/N): ").lower()

    if confirm == 'y':
        if clear_chroma_data():
            print("\nâœ… è¯·é‡å¯åº”ç”¨ä»¥é‡æ–°ç´¢å¼•æ•°æ®")
        else:
            print("âŒ æ¸…ç©ºå¤±è´¥")
    else:
        print("âŒ å·²å–æ¶ˆ")


def launch_app():
    """å¯åŠ¨åº”ç”¨"""
    print_header("å¯åŠ¨åº”ç”¨")

    print("æ­£åœ¨å¯åŠ¨Streamlitåº”ç”¨...")
    print("è¯·åœ¨æ–°ç»ˆç«¯ä¸­è¿è¡Œ: streamlit run app.py")
    print("æˆ–æŒ‰ Ctrl+C è¿”å›èœå•")

    try:
        import subprocess
        subprocess.run(["streamlit", "run", "app.py"])
    except KeyboardInterrupt:
        print("\nè¿”å›èœå•...")


def export_stats():
    """å¯¼å‡ºç»Ÿè®¡ä¿¡æ¯"""
    print_header("å¯¼å‡ºç»Ÿè®¡")

    stats_file = f"./data/stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"

    html_files, json_file = check_data_directory()

    with open(stats_file, 'w', encoding='utf-8') as f:
        f.write("åŒ»ç–—RAGç³»ç»Ÿ - æ•°æ®ç»Ÿè®¡\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("=" * 50 + "\n\n")

        f.write(f"HTMLæ–‡ä»¶æ•°é‡: {len(html_files)}\n")
        f.write("HTMLæ–‡ä»¶åˆ—è¡¨:\n")
        for file in sorted(html_files):
            size = os.path.getsize(f"./data/{file}") / 1024
            f.write(f"  - {file} ({size:.1f} KB)\n")

        f.write("\n")

        if os.path.exists(json_file):
            with open(json_file, 'r', encoding='utf-8') as jf:
                data = json.load(jf)

            f.write(f"JSONæ•°æ®æ¡ç›®: {len(data)}\n")

            from collections import Counter
            sources = Counter([item.get('source_file', 'æœªçŸ¥') for item in data])

            f.write("\nå„æ–‡ä»¶æ–‡æœ¬å—åˆ†å¸ƒ:\n")
            for source, count in sources.most_common():
                f.write(f"  {source}: {count} å—\n")

        f.write("\n" + "=" * 50 + "\n")
        f.write("ç»Ÿè®¡ç»“æŸ\n")

    print(f"âœ… ç»Ÿè®¡ä¿¡æ¯å·²å¯¼å‡ºåˆ°: {stats_file}")


if __name__ == "__main__":
    try:
        main_menu()
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œå†è§ï¼")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºå‡ºé”™: {e}")
        import traceback

        traceback.print_exc()